{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "406947a351412180",
   "metadata": {},
   "source": [
    "# From Seismic Waves to Neural Inference: A Hands-On Introduction to Simulation-Based Inference (SBI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33b3033d8b644a5",
   "metadata": {},
   "source": [
    "## Installation and packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9172aa742bb56dfb",
   "metadata": {},
   "source": [
    "Create a new conda environment:\n",
    "\n",
    "```zsh\n",
    "conda create -n compearth-workshop python=3.11 -y\n",
    "conda activate compearth-workshop\n",
    "```\n",
    "\n",
    "Install jupyter:\n",
    "\n",
    "```zsh\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Link your kernel to the conda environment:\n",
    "\n",
    "```zsh\n",
    "python -m ipykernel install --user --name compearth-workshop --display-name \"CompEarth Workshop\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "!pip install torch sbi matplotlib numpy pandas pathlib\n",
    "!pip install git+https://github.com/nschaetti/CompEarth_Workshop.git --upgrade"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2fb03894",
   "metadata": {},
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "rng = np.random.default_rng(seed)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ca0397ae",
   "metadata": {},
   "source": [
    "!nvidia-smi"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f457f87b",
   "metadata": {},
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1d86d4a430268c70",
   "metadata": {},
   "source": [
    "## Introduction — The Inverse Problem in Earth Sciences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7495f24670272c",
   "metadata": {},
   "source": [
    "### Understanding the inverse problem"
   ]
  },
  {
   "cell_type": "code",
   "id": "9d97d88d33903db8",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5d26786adadd11ce",
   "metadata": {},
   "source": [
    "### The Bayesian view"
   ]
  },
  {
   "cell_type": "code",
   "id": "b783a3e99ad1c55a",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a4c7d0954cc1bca3",
   "metadata": {},
   "source": [
    "### Why Simulation-Based Inference (SBI)?"
   ]
  },
  {
   "cell_type": "code",
   "id": "a1a11caa296fdcb3",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "474df41690be4b3a",
   "metadata": {},
   "source": [
    "### What you will learn today"
   ]
  },
  {
   "cell_type": "code",
   "id": "74cb51da678b2b62",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7fad6a51ab2d672f",
   "metadata": {},
   "source": [
    "## 2 The Forward Model — Simulating Surface Wave Dispersion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4440f728c8203a72",
   "metadata": {},
   "source": [
    "### 2.1 Physical background"
   ]
  },
  {
   "cell_type": "code",
   "id": "2c7292cbad795d6b",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5ce6e779146b8266",
   "metadata": {},
   "source": [
    "### 2.2 Using the simulator `surfdisp2k25`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbec01d53ec544b7",
   "metadata": {},
   "source": [
    "#### Playing with the simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c89d7a75bd8821",
   "metadata": {},
   "source": [
    "HuggingFace Space: https://huggingface.co/spaces/MIGRATE/surfdisp2k25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eea2bb405ef42d",
   "metadata": {},
   "source": [
    "The `surfdisp2k25` simulator computes **Rayleigh-wave dispersion curves**, showing how the **group velocity** of surface waves varies with **period** for a given layered Earth model.\n",
    "\n",
    "Each model is described by a set of physical and numerical parameters.\n",
    "\n",
    "**Physical parameters**\n",
    "\n",
    "* `n_layers` – number of layers, including the half-space at the bottom.\n",
    "* `vpvs` – ratio ($V_p/V_s$), typically between 1.7 and 1.9 for crustal materials.\n",
    "* `thicknesses` – list of layer thicknesses in kilometers. The last layer has a thickness of 0 to represent an infinite half-space.\n",
    "* `vs_layers` – list of shear-wave velocities ($V_s$) in kilometers per second, usually increasing with depth.\n",
    "* `ρ` – density of each layer, estimated here from an empirical relation ($ρ = 0.32 + 0.77 \\times V_p$).\n",
    "\n",
    "All these parameters are concatenated into a single input tensor:\n",
    "\n",
    "$$\n",
    "\\theta = [,n,, V_P/V_S,, h_1, h_2, …, h_{N_{\\max}},, V_{S,1}, V_{S,2}, …, V_{S,N_{\\max}},]\n",
    "$$\n",
    "\n",
    "For a single model, `theta` has shape `(1, 2 + 2*Nmax)`.\n",
    "When simulating several models at once, the batch dimension corresponds to the number of models.\n",
    "\n",
    "**Numerical parameters**\n",
    "\n",
    "* `p_min`, `p_max` – minimum and maximum periods in seconds defining the frequency range.\n",
    "* `kmax` – number of discrete period samples between `p_min` and `p_max`.\n",
    "* `iflsph` – Earth geometry flag: 0 for flat Earth, 1 for spherical correction.\n",
    "* `iwave` – wave type: 1 for Love waves, 2 for Rayleigh waves.\n",
    "* `mode` – mode number: 1 for the fundamental mode.\n",
    "* `igr` – 1 to compute group velocity (dispersion curve), 0 for phase velocity only.\n",
    "\n",
    "The simulator returns a tensor of shape `(B, kmax)` where `B` is the number of models.\n",
    "Each row corresponds to a dispersion curve ($c(T)$) representing group velocity (in km/s) as a function of period (in seconds).\n",
    "\n",
    "We now test the simulator on a simple two-layer model, representing a soft sedimentary layer over a stiffer half-space.\n",
    "\n",
    "| Parameter        | Description                  | Value              |\n",
    "| ---------------- | ---------------------------- | ------------------ |\n",
    "| `n_layers`       | Number of layers             | 2                  |\n",
    "| `vpvs`           | (V_P/V_S) ratio              | 1.75               |\n",
    "| `thicknesses`    | Layer thicknesses (km)       | [2.0, 0.0]         |\n",
    "| `vs_layers`      | Shear-wave velocities (km/s) | [2.5, 3.5]         |\n",
    "| `p_min`, `p_max` | Period range (s)             | [0.5, 20.0]        |\n",
    "| `kmax`           | Number of periods            | 60                 |\n",
    "| `iflsph`         | Earth geometry               | 0 (flat Earth)     |\n",
    "| `iwave`          | Wave type                    | 2 (Rayleigh)       |\n",
    "| `mode`           | Mode number                  | 1 (fundamental)    |\n",
    "| `igr`            | Velocity type                | 1 (group velocity) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a54ee47f00fdb4f",
   "metadata": {},
   "source": [
    "#### 2.2.1. Testing the simulator with a simple two-layer model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b1cde5990b830b",
   "metadata": {},
   "source": [
    "Let’s start by defining a model with **two layers** over a half-space."
   ]
  },
  {
   "cell_type": "code",
   "id": "560cee574829e30f",
   "metadata": {},
   "source": [
    "from compearth.extensions.surfdisp2k25 import dispsurf2k25_simulator\n",
    "\n",
    "# Physical model parameters\n",
    "n_layers = 4                # Number of layers (including the half-space)\n",
    "vpvs = 1.75                 # Vp/Vs ratio (typical for crustal rocks)\n",
    "thicknesses = [0.9, 1.2, 0.34, 0.0]    # Layer thicknesses in km (0 for half-space)\n",
    "vs_layers = [1.5, 1.2, 2.5, 3.5]      # Shear-wave velocities in km/s (increase with depth)\n",
    "\n",
    "# Combine all parameters into θ = [n, vpvs, h..., vs...]\n",
    "theta = torch.tensor([[n_layers, vpvs] + thicknesses + vs_layers], dtype=torch.float32)\n",
    "\n",
    "# Numerical simulation parameters\n",
    "p_min, p_max = 1.0, 5.0    # Period range in seconds\n",
    "kmax = 108                   # Number of discrete period samples\n",
    "iflsph = 0                  # Flat Earth approximation (0 = flat, 1 = spherical)\n",
    "iwave = 2                   # Wave type (2 = Rayleigh, 1 = Love)\n",
    "mode = 1                    # Fundamental mode\n",
    "igr = 1                     # Compute group velocity (1 = group, 0 = phase)\n",
    "\n",
    "print(f\"Theta shape: {theta.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e62d4873b5e39e3d",
   "metadata": {},
   "source": [
    "# Run the simulator\n",
    "disp_curve = dispsurf2k25_simulator(\n",
    "    theta=theta,\n",
    "    p_min=p_min,\n",
    "    p_max=p_max,\n",
    "    kmax=kmax,\n",
    "    iflsph=iflsph,\n",
    "    iwave=iwave,\n",
    "    mode=mode,\n",
    "    igr=igr,\n",
    ")\n",
    "\n",
    "print(\"Dispersion curve shape:\", disp_curve.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "336558bd",
   "metadata": {},
   "source": [
    "disp_curve"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1668ae46",
   "metadata": {},
   "source": [
    "np.linspace(p_min, p_max, disp_curve.shape[1])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "38a52593",
   "metadata": {},
   "source": [
    "theta"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a109c9f96409032e",
   "metadata": {},
   "source": [
    "#### 2.2.2 Visualizing the model and its generated dispersion curve\n",
    "\n",
    "To better understand the simulator output, we use a helper function `plot_velocity_and_dispersion` that displays both the **velocity structure** (shear-wave velocity versus depth) and the resulting **dispersion curve** (group velocity versus period).\n",
    "\n",
    "The function takes as input:\n",
    "\n",
    "* `thicknesses` – list of layer thicknesses in kilometers. The last one can be set to 0 to indicate the half-space of infinite thickness.\n",
    "* `vs_layers` – list of shear-wave velocities (V_S) in kilometers per second, one per layer.\n",
    "* `disp_curve` – the dispersion curve returned by `dispsurf2k25_simulator`, either a `torch.Tensor` or a NumPy array.\n",
    "* `p_min`, `p_max` – minimum and maximum periods defining the frequency range in seconds.\n",
    "* `kmax` – number of discrete periods used to compute the curve.\n",
    "* `fig_size` – scaling factor for the figure size (default = 2).\n",
    "* `dpi` – rendering resolution for the figure (default = 300).\n",
    "\n",
    "The plot on the **left** shows the 1-D velocity model as a step function of depth.\n",
    "Each horizontal segment corresponds to a homogeneous layer.\n",
    "The plot on the **right** shows the simulated Rayleigh-wave group velocity as a function of period.\n",
    "Short periods sample the shallow layers, while long periods are sensitive to deeper structures.\n",
    "\n",
    "We now display both the model and the corresponding dispersion curve for our two-layer example."
   ]
  },
  {
   "cell_type": "code",
   "id": "516eb7c1d5a922cf",
   "metadata": {},
   "source": [
    "from compearth.utils import plot_velocity_and_dispersion\n",
    "\n",
    "# Plot the model and the curve\n",
    "plot_velocity_and_dispersion(\n",
    "    theta=theta,\n",
    "    disp_curve=disp_curve,\n",
    "    p_min=p_min,\n",
    "    p_max=p_max,\n",
    "    kmax=kmax,\n",
    "    dpi=300\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "36b50f5cf54d536a",
   "metadata": {},
   "source": [
    "#### 2.2.3. Generating multiple random models from a prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9931804a76833f",
   "metadata": {},
   "source": [
    "To explore how different subsurface structures affect the dispersion curves, we now generate **random velocity models** using a simple **prior distribution** over the model parameters.\n",
    "\n",
    "The goal is to produce a small set of plausible Earth models that will later be used to train or test an inference network.\n",
    "\n",
    "Each model is defined by the same structure as before:\n",
    "\n",
    "* `n` – number of layers, drawn randomly between 2 and a maximum value (`max_layers`).\n",
    "* `vpvs` – fixed P-to-S velocity ratio (here 1.75).\n",
    "* `h` – random layer thicknesses, uniformly sampled between 0.5 km and 5.0 km.\n",
    "* `vs` – random shear-wave velocities ($V_s$), uniformly sampled between 1.5 km/s and 4.5 km/s.\n",
    "\n",
    "For each model, these values are concatenated into a parameter vector\n",
    "\n",
    "$$\n",
    "\\theta = [n, vpvs, h_1, …, h_{max}, V_{S,1}, …, V_{S,max}]\n",
    "$$\n",
    "\n",
    "We draw several such vectors to obtain a small **ensemble of synthetic models**.\n",
    "Note that the last layer represents the **half-space** (infinite depth), so its thickness must remain zero and its velocity nonzero to represent the solid substrate."
   ]
  },
  {
   "cell_type": "code",
   "id": "ee8ea67d0413771a",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from compearth.utils import sample_models\n",
    "\n",
    "# Sample from the prior\n",
    "theta_prior, z_vnoi = sample_models(\n",
    "    n_samples=8,\n",
    "    layers_min=2,\n",
    "    layers_max=10,\n",
    "    z_min=0.0,\n",
    "    z_max=5.0,\n",
    "    vs_min=1.5,\n",
    "    vs_max=4.5,\n",
    "    thick_min=0.1,\n",
    "    sort_vs=True\n",
    ")\n",
    "theta_prior.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "494942a4",
   "metadata": {},
   "source": [
    "theta_prior"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2e969792",
   "metadata": {},
   "source": [
    "z_vnoi"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1aedb77893b3fd88",
   "metadata": {},
   "source": [
    "#### 2.2.4. Running the simulator on random models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c42af35",
   "metadata": {},
   "source": [
    "We now run the simulator on the randomly sampled models.\n",
    "Each parameter vector in `theta_prior` is passed to `dispsurf2k25_simulator`, which computes the corresponding dispersion curve.\n",
    "The result `disp_curves` is a tensor of shape `(n_samples, kmax)`, where each row represents the group velocity curve ($c(T)$) for one model.\n",
    "We also define the array of sampled periods to use for plotting."
   ]
  },
  {
   "cell_type": "code",
   "id": "579e159e",
   "metadata": {},
   "source": [
    "# Run the simulator on the sampled models\n",
    "disp_curves = dispsurf2k25_simulator(\n",
    "    theta=theta_prior,\n",
    "    p_min=p_min,\n",
    "    p_max=p_max,\n",
    "    kmax=kmax,\n",
    "    iflsph=iflsph,\n",
    "    iwave=iwave,\n",
    "    mode=mode,\n",
    "    igr=igr\n",
    ")\n",
    "periods = np.linspace(p_min, p_max, kmax)\n",
    "disp_curves.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f5ce6b48",
   "metadata": {},
   "source": [
    "theta_prior[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a0b06d26",
   "metadata": {},
   "source": [
    "z_vnoi[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4c7c36fb4b6f3980",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Plot the model and the curve\n",
    "plot_velocity_and_dispersion(\n",
    "    theta=theta_prior,\n",
    "    disp_curve=disp_curves,\n",
    "    z_vnoi=z_vnoi,\n",
    "    p_min=p_min,\n",
    "    p_max=p_max,\n",
    "    kmax=kmax,\n",
    "    dpi=300,\n",
    "    fig_size=1.5\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fed41e38e431517d",
   "metadata": {},
   "source": [
    "*Observation:*\n",
    "\n",
    "Each dispersion curve represents a distinct velocity model.\n",
    "You can already see the **non-uniqueness** of the inverse problem: different subsurface structures can yield similar dispersion curves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889f215dd82180f3",
   "metadata": {},
   "source": [
    "#### 2.2.5. Discussion — Why this matters\n",
    "\n",
    "This simulator encapsulates our **forward physical model**, ($f(\\theta) \\rightarrow x$).\n",
    "The next step will be to **invert** this mapping: given an observed curve ($x$), infer plausible models ($\\theta$).\n",
    "That’s precisely what *Simulation-Based Inference (SBI)* aims to achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa5eae7fd3e5a57",
   "metadata": {},
   "source": [
    "## Generating a small dataset\n",
    "\n",
    "### Defining the problem\n",
    "\n",
    "In this step, we select one random model from the previously sampled set.\n",
    "Each model is represented as a parameter vector ($\\theta$), which encodes:\n",
    "\n",
    "1. the **number of layers** ($n$),\n",
    "2. the **($V_p/V_s$)** ratio,\n",
    "3. the **layer thicknesses** $(h_1, h_2, \\dots )$,\n",
    "4. and the **shear-wave velocities** $(V_{s,1}, V_{s,2}, \\dots )$.\n",
    "\n",
    "This vector ($\\theta$) fully defines the layered Earth model.\n",
    "We’ll inspect it to see how the model parameters are structured before turning it into a continuous velocity profile."
   ]
  },
  {
   "cell_type": "code",
   "id": "14dc7298",
   "metadata": {},
   "source": [
    "from compearth.utils import theta_to_velocity_profile\n",
    "\n",
    "# Take one of the random models\n",
    "theta_example = theta_prior[0]\n",
    "theta_example"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "041919bb",
   "metadata": {},
   "source": [
    "### Conversion to continuous velocity profile\n",
    "\n",
    "We now convert the parametric model ($\\theta$) into a **continuous velocity profile** ($V_s(z)$),\n",
    "that is, a regularly sampled velocity map as a function of depth.\n",
    "\n",
    "The function `theta_to_velocity_profile` performs the following steps:\n",
    "\n",
    "* it takes a vector ($\\theta$) as input,\n",
    "* reconstructs the layer boundaries from their thicknesses,\n",
    "* assigns each depth interval its corresponding shear velocity ($V_s$),\n",
    "* and returns two arrays:\n",
    "\n",
    "  * `depth`: the depth samples (in km),\n",
    "  * `vs_profile`: the corresponding ($V_s$) values.\n",
    "\n",
    "Here, we sample 60 points uniformly between 0 and 60 km depth.\n",
    "The resulting table shows how shear-wave velocity varies with depth for this particular model."
   ]
  },
  {
   "cell_type": "code",
   "id": "d69f2872d41417cc",
   "metadata": {},
   "source": [
    "# Convert to continuous velocity profile\n",
    "depth, vs_profile = theta_to_velocity_profile(\n",
    "    theta_example, \n",
    "    depth_max=5.0, \n",
    "    n_points=60\n",
    ")\n",
    "print(f\"Depth shape: {depth.shape}\")\n",
    "print(f\"Vs profile shape: {vs_profile.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b061234b",
   "metadata": {},
   "source": [
    "# Display depth and Vs in a small table\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(6, 4), dpi=300)\n",
    "plt.scatter(depth, vs_profile, color='crimson', s=25, marker='o', zorder=3, label='Sample points')\n",
    "plt.ylabel(\"Vs [km/s]\")\n",
    "plt.xlabel(\"Depth [km]\")\n",
    "plt.title(\"Continuous velocity profile from θ\")\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f1971b9",
   "metadata": {},
   "source": [
    "# Generate dataset\n",
    "# n_samples = 10_000\n",
    "n_samples = 100\n",
    "max_layers = 10\n",
    "z_min = 0.0\n",
    "z_max = 5.0\n",
    "vs_min = 0.5\n",
    "vs_max = 4.0\n",
    "thick_min = 0.1\n",
    "n_points_depth = 60\n",
    "kmax = 108\n",
    "\n",
    "# Sampling range\n",
    "p_min, p_max = 1.0, 15.0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5be2bbec",
   "metadata": {},
   "source": [
    "theta_models, z_vnoi = sample_models(\n",
    "    n_samples=n_samples,\n",
    "    layers_min=2,\n",
    "    layers_max=max_layers,\n",
    "    z_min=0.0,\n",
    "    z_max=z_max,\n",
    "    vs_min=vs_min,\n",
    "    vs_max=vs_max,\n",
    "    thick_min=thick_min,\n",
    "    sort_vs=True\n",
    ")\n",
    "print(f\"Theta models: {theta_models.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6a67965f",
   "metadata": {},
   "source": [
    "# Run simulator to get dispersion curves\n",
    "disp_curves = dispsurf2k25_simulator(\n",
    "    theta=theta_models,\n",
    "    p_min=p_min,\n",
    "    p_max=p_max,\n",
    "    kmax=kmax,\n",
    "    iflsph=iflsph,\n",
    "    iwave=iwave,\n",
    "    mode=mode,\n",
    "    igr=igr,\n",
    "    progress=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6ec7e827",
   "metadata": {},
   "source": [
    "# Build velocity profiles\n",
    "vel_maps = []\n",
    "for i in range(n_samples):\n",
    "    _, vs_profile = theta_to_velocity_profile(\n",
    "        theta_models[i],\n",
    "        depth_max=z_max,\n",
    "        n_points=n_points_depth\n",
    "    )\n",
    "    vel_maps.append(vs_profile)\n",
    "# end for"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "866fcdee",
   "metadata": {},
   "source": [
    "vel_maps = np.stack(vel_maps, axis=0)   # shape: (B, 60)\n",
    "z = np.linspace(0, z_max, n_points_depth)\n",
    "periods = np.linspace(p_min, p_max, kmax)\n",
    "\n",
    "# ---- 4. Convert to torch tensors ----\n",
    "theta = torch.tensor(vel_maps, dtype=torch.float32)   # (B, 60)\n",
    "x = disp_curves.to(torch.float32)                     # (B, 108)\n",
    "z = torch.tensor(z, dtype=torch.float32)              # (60,)\n",
    "periods = torch.tensor(periods, dtype=torch.float32)  # (108,)                                     # shape (N, 22)\n",
    "\n",
    "# ---- 5. Display summary ----\n",
    "print(f\"theta (velocity maps): {tuple(theta.shape)}\")\n",
    "print(f\"x (dispersion curves): {tuple(x.shape)}\")\n",
    "print(f\"z (depth samples):     {tuple(z.shape)}\")\n",
    "print(f\"periods:               {tuple(periods.shape)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dbaee6db",
   "metadata": {},
   "source": [
    "print(f\"z: {z}\")\n",
    "print(f\"periods: {periods}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3f9f6483aa402fef",
   "metadata": {},
   "source": [
    "## Learning the Inverse Model — Neural Posterior Estimation (NPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edce9bfc39526c15",
   "metadata": {},
   "source": [
    "### Conceptual overview"
   ]
  },
  {
   "cell_type": "code",
   "id": "62c08fb338eebf3a",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "77cef879e36cf6d9",
   "metadata": {},
   "source": [
    "### Setting up the SBI pipeline\n",
    "\n",
    "#### Defining the prior"
   ]
  },
  {
   "cell_type": "code",
   "id": "78e9f2e0af5cfc8c",
   "metadata": {},
   "source": [
    "from sbi.utils import BoxUniform\n",
    "\n",
    "# Define the prior used for generation\n",
    "prior = BoxUniform(\n",
    "    low=torch.full((60,), vs_min, device=device),\n",
    "    high=torch.full((60,), vs_max, device=device)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0df99238",
   "metadata": {},
   "source": [
    "#### Defining the model and adding data"
   ]
  },
  {
   "cell_type": "code",
   "id": "383b75e4",
   "metadata": {},
   "source": [
    "from sbi.inference import SNPE\n",
    "\n",
    "inference = SNPE(\n",
    "    prior=prior,\n",
    "    density_estimator=\"maf\",\n",
    "    device=device\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7a9ef835",
   "metadata": {},
   "source": [
    "theta = theta.to(device)\n",
    "x = x.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af779247",
   "metadata": {},
   "source": [
    "inference = inference.append_simulations(theta, x)\n",
    "inference"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "607b1dbb",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "id": "8e727717",
   "metadata": {},
   "source": [
    "batch_size = 1024\n",
    "learning_rate = 1e-4\n",
    "validation_fraction = 0.1\n",
    "stop_after_epochs = 200\n",
    "max_num_epochs = 1000\n",
    "show_train_summary = True\n",
    "\n",
    "density_estimator = inference.train(\n",
    "    training_batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    validation_fraction=validation_fraction,\n",
    "    stop_after_epochs=stop_after_epochs,\n",
    "    max_num_epochs=max_num_epochs,\n",
    "    show_train_summary=show_train_summary,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "db4e84ad",
   "metadata": {},
   "source": [
    "# inference._summary[\"training_log_probs\"]\n",
    "# inference._summary[\"validation_log_probs\"]\n",
    "print(f\"Epochs trained: {inference._summary['epochs_trained'][0]}\")\n",
    "print(f\"Best validation loss: {inference._summary['best_validation_loss'][0]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8da02ab5",
   "metadata": {},
   "source": [
    "#### Building the posterior"
   ]
  },
  {
   "cell_type": "code",
   "id": "9e0f0f58",
   "metadata": {},
   "source": [
    "# Build posterior\n",
    "posterior = inference.build_posterior(density_estimator)\n",
    "print(posterior)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "48100758",
   "metadata": {},
   "source": [
    "#### Visualising training and validation curves"
   ]
  },
  {
   "cell_type": "code",
   "id": "5e837508",
   "metadata": {},
   "source": [
    "from compearth.utils import plot_training_summary\n",
    "plot_training_summary(inference)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6aa1ac92",
   "metadata": {},
   "source": [
    "### Training with a bigger dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4994299",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "id": "fa4561af",
   "metadata": {},
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import pickle\n",
    "\n",
    "repo_id = \"MIGRATE/Dispsurf96-Roccastrada-10k\"\n",
    "\n",
    "path = hf_hub_download(repo_id=repo_id, filename=\"data.pkl\", repo_type=\"dataset\")\n",
    "with open(path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "# end with\n",
    "\n",
    "path = hf_hub_download(repo_id=repo_id, filename=\"val_data.pkl\", repo_type=\"dataset\")\n",
    "with open(path, \"rb\") as f:\n",
    "    val_data = pickle.load(f)\n",
    "# end with"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c605140c",
   "metadata": {},
   "source": [
    "theta = data['theta']\n",
    "x = data['x']\n",
    "z = data['z']\n",
    "periods = data['periods']\n",
    "prior = data['prior']\n",
    "\n",
    "vs_min = prior['vs'][0]\n",
    "vs_max = prior['vs'][1]\n",
    "z_min = prior['z'][0]\n",
    "z_max = prior['z'][1]\n",
    "layer_min = prior['layers'][0]\n",
    "layer_max = prior['layers'][1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64b9b3c6",
   "metadata": {},
   "source": [
    "print(\"Training:\")\n",
    "print(f\"theta: {theta.shape}\")\n",
    "print(f\"x: {x.shape}\")\n",
    "print(f\"z: {z.shape}\")\n",
    "print(f\"periods: {periods.shape}\")\n",
    "print(f\"prior: {prior}\")\n",
    "\n",
    "print(\"Validation:\")\n",
    "print(f\"theta: {val_data['theta'].shape}\")\n",
    "print(f\"x: {val_data['x'].shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "243f7ebb",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "dd9f5e3d",
   "metadata": {},
   "source": [
    "from sbi.inference import SNPE\n",
    "\n",
    "# Define the prior used for generation\n",
    "prior = BoxUniform(\n",
    "    low=torch.full((60,), vs_min, device=device),\n",
    "    high=torch.full((60,), vs_max, device=device)\n",
    ")\n",
    "\n",
    "inference = SNPE(\n",
    "    prior=prior,\n",
    "    density_estimator=\"maf\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "theta = theta.to(device)\n",
    "x = x.to(device)\n",
    "\n",
    "inference = inference.append_simulations(theta, x)\n",
    "\n",
    "batch_size = 1024\n",
    "learning_rate = 1e-4\n",
    "validation_fraction = 0.2\n",
    "stop_after_epochs = 300\n",
    "max_num_epochs = 3000\n",
    "show_train_summary = True\n",
    "\n",
    "density_estimator = inference.train(\n",
    "    training_batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    validation_fraction=validation_fraction,\n",
    "    stop_after_epochs=stop_after_epochs,\n",
    "    max_num_epochs=max_num_epochs,\n",
    "    show_train_summary=show_train_summary\n",
    ")\n",
    "\n",
    "# Build posterior\n",
    "posterior = inference.build_posterior(density_estimator)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8abed27",
   "metadata": {},
   "source": [
    "# inference._summary[\"training_log_probs\"]\n",
    "# inference._summary[\"validation_log_probs\"]\n",
    "print(f\"Epochs trained: {inference._summary['epochs_trained'][0]}\")\n",
    "print(f\"Best validation loss: {inference._summary['best_validation_loss'][0]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f420a66a",
   "metadata": {},
   "source": [
    "plot_training_summary(inference)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b33cfdecbccce36c",
   "metadata": {},
   "source": [
    "### Visualizing the learned posterior"
   ]
  },
  {
   "cell_type": "code",
   "id": "42164b3b",
   "metadata": {},
   "source": [
    "theta_val = data['theta']\n",
    "x_val = data['x']\n",
    "\n",
    "theta_val = theta_val.to(device)\n",
    "x_val = x_val.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "13fd2b24364695ee",
   "metadata": {},
   "source": [
    "obs_idx = 6\n",
    "x_obs = x_val[obs_idx:obs_idx+1, :]\n",
    "print(f\"x_obs: {x_obs.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44bcb1d0",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 3), dpi=300)\n",
    "\n",
    "axes[1].plot(periods, x_obs[0].cpu(), \"blue\", linewidth=2, alpha=1)\n",
    "axes[1].set_xlim(p_min, p_max)\n",
    "axes[1].set_xlabel(\"Period [T]\")\n",
    "axes[1].set_ylabel(\"Vg [km/s]\")\n",
    "axes[1].set_title(f\"Dispersion curve (Vg) for observation {obs_idx}\")\n",
    "axes[1].grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "axes[0].plot(z, theta_val[obs_idx].cpu(), 'red', linewidth=2)\n",
    "axes[0].set_xlim(0, z_max)\n",
    "axes[0].set_ylim(vs_min, vs_max)\n",
    "axes[0].set_xlabel(\"Depth [km]\")\n",
    "axes[0].set_ylabel(\"Vs [km/s]\")\n",
    "axes[0].set_title(f\"Velocity map (Vs) for model {obs_idx}\")\n",
    "axes[0].grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b8011682",
   "metadata": {},
   "source": [
    "samples = posterior.sample((100,), x=x_obs)\n",
    "print(f\"Sample shape: {samples.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ae3d0180",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "for i in range(100):\n",
    "    plt.plot(z, samples[i].cpu(), linewidth=1, alpha=0.5)\n",
    "# end for\n",
    "plt.plot(z, theta_val[obs_idx].cpu(), 'red', linewidth=2)\n",
    "plt.xlim(0, z_max)\n",
    "plt.ylim(vs_min, vs_max)\n",
    "plt.xlabel(\"Depth [km]\")\n",
    "plt.ylabel(\"Vs [km/s]\")\n",
    "plt.title(f\"Posterior samples and ground truth for model {obs_idx}\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c839812d",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "from compearth.utils import plot_posterior_grid\n",
    "plot_posterior_grid(\n",
    "    posterior=posterior,\n",
    "    x=x_val,\n",
    "    theta=theta_val,\n",
    "    z=z,\n",
    "    n_row=5,\n",
    "    n_col=3,\n",
    "    n_samples=100,\n",
    "    vs_min=vs_min,\n",
    "    vs_max=vs_max,\n",
    "    figsize=(9, 12),\n",
    "    device=device\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "760003c3",
   "metadata": {},
   "source": [
    "### Infering flat models"
   ]
  },
  {
   "cell_type": "code",
   "id": "97812ccc",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# === 6. Visualisation ===\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Posterior samples (profils Vs)\n",
    "plt.plot(z, samples[0].cpu().numpy(), alpha=1, color=\"blue\", label=\"Posterior sample\")\n",
    "\n",
    "# Vrai profil en rouge épais\n",
    "plt.plot(z, theta_val[obs_idx].cpu().numpy(), color=\"red\", linewidth=2, label=\"Ground truth\")\n",
    "plt.ylim(vs_min - 0.1, vs_max + 0.1)\n",
    "plt.xlim(z.min(), z.max())\n",
    "plt.xlabel(\"Depth (z)\")\n",
    "plt.ylabel(\"Vs\")\n",
    "plt.title(\"A single posterior sample vs ground truth\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "37c6ab45",
   "metadata": {},
   "source": [
    "import ruptures as rpt\n",
    "\n",
    "sample = samples[0].cpu().numpy()\n",
    "\n",
    "algo = rpt.Pelt(model=\"l2\").fit(sample)\n",
    "bkps = algo.predict(pen=5)\n",
    "vs_flat = np.zeros_like(sample)\n",
    "start = 0\n",
    "for end in bkps:\n",
    "    print(f\"Region found with range {start}-{end}\")\n",
    "    vs_flat[start:end] = np.mean(sample[start:end])\n",
    "    start = end\n",
    "# end for\n",
    "\n",
    "# Posterior samples (profils Vs)\n",
    "plt.figure(figsize=(8, 6), dpi=300)\n",
    "plt.plot(z, theta_val[obs_idx].cpu().numpy(), color=\"red\", linewidth=3, label=\"Ground truth\", alpha=0.2)\n",
    "plt.plot(z, samples[0].cpu().numpy(), alpha=0.5, color=\"blue\", label=\"Posterior sample\", linewidth=1)\n",
    "plt.plot(z, vs_flat, alpha=1, color=\"orange\", label=\"Flatten posterior sample\", linewidth=2)\n",
    "plt.ylim(vs_min - 0.1, vs_max + 0.1)\n",
    "plt.xlim(z.min(), z.max())\n",
    "plt.xlabel(\"Depth (z)\")\n",
    "plt.ylabel(\"Vs\")\n",
    "plt.title(\"Posterior (sampling Vs) vs ground truth\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5d7a28fe",
   "metadata": {},
   "source": [
    "print(theta_val.shape)\n",
    "print(z.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5c37b9cf",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from compearth.utils import plot_flatten_grid\n",
    "\n",
    "# Show the effect of different values \n",
    "# for the penality parameter\n",
    "plot_flatten_grid(\n",
    "    sample=samples[0].cpu().numpy(),\n",
    "    theta=theta_val[obs_idx].cpu(),\n",
    "    z=z.cpu(),\n",
    "    n_row=5,\n",
    "    n_col=3,\n",
    "    penalty_min=0,\n",
    "    penalty_max=15,\n",
    "    vs_min=vs_min,\n",
    "    vs_max=vs_max,\n",
    "    figsize=(8, 11)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "224d6100",
   "metadata": {},
   "source": [
    "from typing import Union\n",
    "\n",
    "def plot_flatten_models(\n",
    "    samples: Union[np.ndarray, torch.Tensor],\n",
    "    theta: Union[np.ndarray, torch.Tensor],\n",
    "    z: Union[np.ndarray, torch.Tensor],\n",
    "    penalty: float = 1.0,\n",
    "    vs_min: float = 1.5,\n",
    "    vs_max: float = 4.5,\n",
    "    figsize: tuple = (8, 5),\n",
    "    dpi: int = 200,\n",
    "):\n",
    "    \"\"\"\n",
    "    Flatten all posterior samples using PELT with a fixed penalty\n",
    "    and display them together with the ground truth profile.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    samples : np.ndarray or torch.Tensor\n",
    "        Posterior samples (N, D_z)\n",
    "    theta : np.ndarray or torch.Tensor\n",
    "        Ground truth velocity profile (D_z,)\n",
    "    z : np.ndarray or torch.Tensor\n",
    "        Depth coordinates (D_z,)\n",
    "    penalty : float\n",
    "        Penalty value for PELT algorithm.\n",
    "    vs_min, vs_max : float\n",
    "        Limits for Vs axis.\n",
    "    figsize : tuple\n",
    "        Figure size.\n",
    "    dpi : int\n",
    "        Plot resolution.\n",
    "    \"\"\"\n",
    "    # --- Convert tensors to numpy ---\n",
    "    if hasattr(samples, \"detach\"):\n",
    "        samples = samples.detach().cpu().numpy()\n",
    "    if hasattr(theta, \"detach\"):\n",
    "        theta = theta.detach().cpu().numpy()\n",
    "    if hasattr(z, \"detach\"):\n",
    "        z = z.detach().cpu().numpy()\n",
    "\n",
    "    n_samples = samples.shape[0]\n",
    "\n",
    "    # --- Prepare figure ---\n",
    "    plt.figure(figsize=figsize, dpi=dpi)\n",
    "\n",
    "    # Plot flattened posterior samples\n",
    "    for i in range(n_samples):\n",
    "        s = samples[i]\n",
    "        algo = rpt.Pelt(model=\"l2\").fit(s)\n",
    "        bkps = algo.predict(pen=penalty)\n",
    "\n",
    "        vs_flat = np.zeros_like(s)\n",
    "        start = 0\n",
    "        for end in bkps:\n",
    "            vs_flat[start:end] = np.mean(s[start:end])\n",
    "            start = end\n",
    "\n",
    "        plt.plot(z, vs_flat, alpha=0.3, linewidth=1)\n",
    "    # end for\n",
    "\n",
    "    # Plot ground truth\n",
    "    plt.plot(z, theta, color=\"red\", linewidth=2, label=\"Ground truth\", alpha=0.7)\n",
    "\n",
    "    # Style\n",
    "    plt.xlabel(\"Depth [km]\")\n",
    "    plt.ylabel(\"Vs [km/s]\")\n",
    "    plt.title(f\"Flattened posterior samples (penalty={penalty:.2f})\")\n",
    "    plt.ylim(vs_min - 0.1, vs_max + 0.1)\n",
    "    plt.xlim(z.min(), z.max())\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "# end def plot_flatten_models"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0edb3dad",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "plot_flatten_models(\n",
    "    samples=samples[:20],   # par exemple 50 échantillons\n",
    "    theta=theta_val[obs_idx],\n",
    "    z=z,\n",
    "    penalty=0.0,\n",
    "    vs_min=vs_min,\n",
    "    vs_max=vs_max,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6d05790a",
   "metadata": {},
   "source": [
    "def plot_random_flatten_models(\n",
    "    samples: Union[np.ndarray, torch.Tensor],\n",
    "    theta: Union[np.ndarray, torch.Tensor],\n",
    "    z: Union[np.ndarray, torch.Tensor],\n",
    "    penalty_min: float = 0.1,\n",
    "    penalty_max: float = 5.0,\n",
    "    vs_min: float = 1.5,\n",
    "    vs_max: float = 4.5,\n",
    "    figsize: tuple = (8, 5),\n",
    "    dpi: int = 200,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Flatten each posterior sample with a random penalty drawn uniformly\n",
    "    between `penalty_min` and `penalty_max`, and display all results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    samples : np.ndarray or torch.Tensor\n",
    "        Posterior samples (N, D_z)\n",
    "    theta : np.ndarray or torch.Tensor\n",
    "        Ground truth velocity profile (D_z,)\n",
    "    z : np.ndarray or torch.Tensor\n",
    "        Depth coordinates (D_z,)\n",
    "    penalty_min, penalty_max : float\n",
    "        Range for randomly sampled penalties.\n",
    "    vs_min, vs_max : float\n",
    "        Velocity axis limits.\n",
    "    figsize : tuple\n",
    "        Figure size.\n",
    "    dpi : int\n",
    "        Plot resolution.\n",
    "    seed : int\n",
    "        Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    # --- Convert tensors to numpy ---\n",
    "    if hasattr(samples, \"detach\"):\n",
    "        samples = samples.detach().cpu().numpy()\n",
    "    if hasattr(theta, \"detach\"):\n",
    "        theta = theta.detach().cpu().numpy()\n",
    "    if hasattr(z, \"detach\"):\n",
    "        z = z.detach().cpu().numpy()\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_samples = samples.shape[0]\n",
    "\n",
    "    plt.figure(figsize=figsize, dpi=dpi)\n",
    "\n",
    "    # --- Process each sample ---\n",
    "    for i in range(n_samples):\n",
    "        s = samples[i]\n",
    "        penalty = rng.uniform(penalty_min, penalty_max)\n",
    "\n",
    "        algo = rpt.Pelt(model=\"l2\").fit(s)\n",
    "        bkps = algo.predict(pen=penalty)\n",
    "\n",
    "        vs_flat = np.zeros_like(s)\n",
    "        start = 0\n",
    "        for end in bkps:\n",
    "            vs_flat[start:end] = np.mean(s[start:end])\n",
    "            start = end\n",
    "\n",
    "        plt.plot(z, vs_flat, alpha=0.3, linewidth=1)\n",
    "    # end for\n",
    "\n",
    "    # Plot ground truth\n",
    "    plt.plot(z, theta, color=\"red\", linewidth=2, alpha=0.7, label=\"Ground truth\")\n",
    "\n",
    "    plt.xlabel(\"Depth [km]\")\n",
    "    plt.ylabel(\"Vs [km/s]\")\n",
    "    plt.title(f\"Random flattening (penalty ∈ [{penalty_min}, {penalty_max}])\")\n",
    "    plt.ylim(vs_min - 0.1, vs_max + 0.1)\n",
    "    plt.xlim(z.min(), z.max())\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "# end def plot_random_flatten_models"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c6e69230",
   "metadata": {},
   "source": [
    "plot_random_flatten_models(\n",
    "    samples=samples[:50],\n",
    "    theta=theta_val[obs_idx],\n",
    "    z=z,\n",
    "    penalty_min=0.0,\n",
    "    penalty_max=10.0,\n",
    "    vs_min=vs_min,\n",
    "    vs_max=vs_max,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ffb76f1",
   "metadata": {},
   "source": [
    "def flatten_models(\n",
    "    samples: Union[np.ndarray, torch.Tensor],\n",
    "    penalty: float = 1.0,\n",
    "    model: str = \"l2\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Flatten posterior samples using PELT segmentation with a fixed penalty.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    samples : np.ndarray or torch.Tensor\n",
    "        Posterior samples of shape (N, D_z)\n",
    "    penalty : float\n",
    "        Penalty value for PELT algorithm.\n",
    "    model : str\n",
    "        Cost model for ruptures (default: \"l2\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vs_flat_all : np.ndarray\n",
    "        Flattened velocity profiles of shape (N, D_z)\n",
    "    \"\"\"\n",
    "    # --- Convert to numpy ---\n",
    "    if hasattr(samples, \"detach\"):\n",
    "        samples = samples.detach().cpu().numpy()\n",
    "\n",
    "    n_samples, depth_points = samples.shape\n",
    "    vs_flat_all = np.zeros_like(samples)\n",
    "\n",
    "    # --- Apply PELT to each posterior sample ---\n",
    "    for i in range(n_samples):\n",
    "        s = samples[i]\n",
    "        algo = rpt.Pelt(model=model).fit(s)\n",
    "        bkps = algo.predict(pen=penalty)\n",
    "\n",
    "        vs_flat = np.zeros_like(s)\n",
    "        start = 0\n",
    "        for end in bkps:\n",
    "            vs_flat[start:end] = np.mean(s[start:end])\n",
    "            start = end\n",
    "\n",
    "        vs_flat_all[i] = vs_flat\n",
    "    # end for\n",
    "\n",
    "    return vs_flat_all\n",
    "# end def flatten_models"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cea9d211",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "vs_flat_all = flatten_models(samples=samples, penalty=1.0)\n",
    "print(vs_flat_all.shape)  # (100, D_z)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d0570bf",
   "metadata": {},
   "source": [
    "def random_flatten_models(\n",
    "    samples: Union[np.ndarray, torch.Tensor],\n",
    "    penalty_min: float = 0.1,\n",
    "    penalty_max: float = 5.0,\n",
    "    model: str = \"l2\",\n",
    "    seed: int = 42,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Flatten posterior samples using PELT segmentation, with a random penalty\n",
    "    drawn uniformly between `penalty_min` and `penalty_max` for each sample.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    samples : np.ndarray or torch.Tensor\n",
    "        Posterior samples of shape (N, D_z)\n",
    "    penalty_min, penalty_max : float\n",
    "        Range of random penalties for the PELT algorithm.\n",
    "    model : str\n",
    "        Cost model for ruptures (default: \"l2\").\n",
    "    seed : int\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vs_flat_all : np.ndarray\n",
    "        Flattened velocity profiles of shape (N, D_z)\n",
    "    \"\"\"\n",
    "    # --- Convert to numpy ---\n",
    "    if hasattr(samples, \"detach\"):\n",
    "        samples = samples.detach().cpu().numpy()\n",
    "\n",
    "    n_samples, depth_points = samples.shape\n",
    "    vs_flat_all = np.zeros_like(samples)\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # --- Flatten each sample with a random penalty ---\n",
    "    for i in range(n_samples):\n",
    "        s = samples[i]\n",
    "        penalty = rng.uniform(penalty_min, penalty_max)\n",
    "\n",
    "        algo = rpt.Pelt(model=model).fit(s)\n",
    "        bkps = algo.predict(pen=penalty)\n",
    "\n",
    "        vs_flat = np.zeros_like(s)\n",
    "        start = 0\n",
    "        for end in bkps:\n",
    "            vs_flat[start:end] = np.mean(s[start:end])\n",
    "            start = end\n",
    "\n",
    "        vs_flat_all[i] = vs_flat\n",
    "    # end for\n",
    "\n",
    "    return vs_flat_all\n",
    "# end def random_flatten_models"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "65e64b8f",
   "metadata": {},
   "source": [
    "vs_flat_rand = random_flatten_models(\n",
    "    samples=samples,\n",
    "    penalty_min=0.0,\n",
    "    penalty_max=10.0,\n",
    "    seed=123,\n",
    ")\n",
    "print(vs_flat_rand.shape)  # (100, D_z)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3f076585",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "4ba9e623",
   "metadata": {},
   "source": [
    "# torch.save({\n",
    "#     \"model_type\": type(inference).__name__,\n",
    "#     \"neural_net_state_dict\": inference._neural_net.state_dict(),\n",
    "#     \"prior\": inference._prior,\n",
    "# }, \"test.pt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f1a78505",
   "metadata": {},
   "source": [
    "# checkpoint = torch.load(\"test.pt\", map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "# inference = SNPE(checkpoint[\"prior\"])\n",
    "# inference._neural_net.load_state_dict(checkpoint[\"neural_net_state_dict\"])\n",
    "\n",
    "# posterior = inference.build_posterior()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e452a9a2cabe34e9",
   "metadata": {},
   "source": [
    "## Working with Pre-Trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba94ccc39f9868c",
   "metadata": {},
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "code",
   "id": "b119380ca45c74a3",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a52a4a2d654dce75",
   "metadata": {},
   "source": [
    "### Loading a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "id": "9f213c37a3ee39ac",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2895dca4a2d36c58",
   "metadata": {},
   "source": [
    "### Sampling and visualization"
   ]
  },
  {
   "cell_type": "code",
   "id": "69d5bab156eaa5b9",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b9094bc9c795bab5",
   "metadata": {},
   "source": [
    "## Model Validation and Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f8a6a",
   "metadata": {},
   "source": [
    "### Log-probability\n",
    "\n",
    "Once the posterior distribution has been trained, there are **two complementary ways to use it**:\n",
    "\n",
    "1. **Sampling** from the posterior: given an observed dispersion curve $x_obs$, we can draw samples from $p(\\theta \\mid x_{obs})$. Each sample corresponds to a possible **Earth model** — a velocity profile consistent with the observed data. Sampling allows us to *visualize uncertainty*: it shows how many different models could explain the same observation.\n",
    "\n",
    "2. **Evaluating the posterior density**: conversely, we can compute the **log-probability** of a specific model $\\theta$ given the observation $x_{obs}$:\n",
    "\n",
    "   $$\n",
    "   \\large\n",
    "   \\log p(\\theta \\mid x_{obs})\n",
    "   $$\n",
    "   \n",
    "   This value tells us **how plausible** that model is, according to the learned posterior. The higher (less negative) the log-probability, the more confident the model is that $\\theta$ could have generated the data $x_{obs}$.\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "Think of the posterior as a *landscape of plausibility*:\n",
    "\n",
    "* **Sampling** means picking random points from this landscape, exploring regions of high probability.\n",
    "* **Evaluating the log-probability** means checking how \"high\" or \"low\" a given point lies in this landscape.\n",
    "\n",
    "Mathematically, the log-probability corresponds to the logarithm of the posterior density at a specific location $\\theta$.\n",
    "It is often preferred over the probability itself because densities can be extremely small, and the log-scale gives a more stable numerical measure.\n",
    "\n",
    "#### How to interpret log-probability values\n",
    "\n",
    "* **High (less negative)** → The posterior assigns *high plausibility* to this model: it fits the data $x$ well.\n",
    "* **Low (very negative)** → The posterior considers this model *unlikely* under the given observation.\n",
    "* **Average log-probability** over many samples → A useful quantitative measure of posterior quality.\n",
    "  If the average log-probability is too low, the posterior might be **overconfident** (too narrow) or **miscalibrated**.\n",
    "\n",
    "#### How the log-probability is computed\n",
    "\n",
    "Voici la version Markdown prête à copier dans ton notebook Jupyter, avec les formules correctement formatées :\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ How the log-probability is computed\n",
    "\n",
    "Under the hood, the posterior is represented by a **normalizing flow** — an *invertible neural network* that transforms a simple base distribution (usually a standard Gaussian) into a complex, structured one that matches the true posterior $p(\\theta \\mid x)$.\n",
    "\n",
    "Because this transformation ( f ) is **invertible** and **differentiable**, the probability density of a sample $\\theta$ can be computed exactly using the **change of variables** formula:\n",
    "\n",
    "$$\n",
    "p(\\theta \\mid x) = p_z(f^{-1}(\\theta)) \\Big| \\det J_{f^{-1}}(\\theta) \\Big|\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $p_z$ is the base density (often $\\mathcal{N}(0, I)$),\n",
    "* $f^{-1}(\\theta)$ maps the parameter sample back into the latent space,\n",
    "* and $J_{f^{-1}}(\\theta)$ is the Jacobian of the inverse transformation.\n",
    "\n",
    "Taking the logarithm gives the **log-probability**:\n",
    "\n",
    "$$\n",
    "\\log p(\\theta \\mid x) = \\log p_z(f^{-1}(\\theta)) + \\log \\Big| \\det J_{f^{-1}}(\\theta) \\Big|\n",
    "$$\n",
    "\n",
    "This property is what makes normalizing flows so powerful:\n",
    "they allow both **sampling** (by applying $f$) and **density evaluation** (via $f^{-1}$) in a mathematically consistent way.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ebd4eac6",
   "metadata": {},
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Let's compute the average log-prob for 20 samples\n",
    "n_samples = 20\n",
    "\n",
    "total_log_probs = 0.0\n",
    "for i in tqdm(range(n_samples), desc=\"Computing log-prob\"):\n",
    "    log_p = posterior.log_prob(theta_val[i:i+1], x=x_val[i:i+1])\n",
    "    total_log_probs += log_p.cpu().item()\n",
    "# end for \n",
    "\n",
    "# Log. prob\n",
    "print(f\"Log-prob: {total_log_probs/n_samples}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d5ea815c2a27ed71",
   "metadata": {},
   "source": [
    "### Posterior Predictive Check (PPC)"
   ]
  },
  {
   "cell_type": "code",
   "id": "f3edd79e",
   "metadata": {},
   "source": [
    "from typing import Union, Tuple\n",
    "\n",
    "def posterior_to_theta(\n",
    "    z: Union[np.ndarray, torch.Tensor],\n",
    "    vs_batch: Union[np.ndarray, torch.Tensor],\n",
    "    vpvs: float = 1.75,\n",
    "    penalty_min: float = 0.1,\n",
    "    penalty_max: float = 5.0,\n",
    "    model: str = \"l2\",\n",
    "    max_layers: int = 20,\n",
    "    seed: int = 42,\n",
    ") -> Tuple[torch.Tensor, list, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Convert multiple posterior velocity samples into layered Earth models (θ),\n",
    "    using PELT segmentation with a random penalty drawn uniformly for each sample.\n",
    "    Breakpoints are returned in depth (km) rather than indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : np.ndarray or torch.Tensor\n",
    "        Depth coordinates (D_z,), in km.\n",
    "    vs_batch : np.ndarray or torch.Tensor\n",
    "        Velocity profiles (N, D_z)\n",
    "    vpvs : float\n",
    "        Fixed Vp/Vs ratio for all models\n",
    "    penalty_min, penalty_max : float\n",
    "        Range for random penalties\n",
    "    model : str\n",
    "        Cost model for ruptures (default: 'l2')\n",
    "    max_layers : int\n",
    "        Maximum number of layers (for padding in θ)\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    theta_all : torch.Tensor\n",
    "        Model parameters of shape (N, 2 + 2 * max_layers)\n",
    "        [n_layers, vpvs, h_1...h_max, vs_1...vs_max]\n",
    "    bkps_all_km : list[list[float]]\n",
    "        List of breakpoint depths (km) for each sample\n",
    "    penalties : np.ndarray\n",
    "        Penalties drawn for each sample\n",
    "    \"\"\"\n",
    "    # --- Convert to numpy ---\n",
    "    if hasattr(vs_batch, \"detach\"):\n",
    "        vs_batch = vs_batch.detach().cpu().numpy()\n",
    "    # end if\n",
    "    \n",
    "    if hasattr(z, \"detach\"):\n",
    "        z = z.detach().cpu().numpy()\n",
    "    # end if\n",
    "\n",
    "    n_samples, n_depths = vs_batch.shape\n",
    "    theta_all = []\n",
    "    bkps_all_km = []\n",
    "    penalties = []\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        vs = vs_batch[i]\n",
    "        penalty = rng.uniform(penalty_min, penalty_max)\n",
    "        penalties.append(penalty)\n",
    "\n",
    "        algo = rpt.Pelt(model=model).fit(vs)\n",
    "        bkps = algo.predict(pen=penalty)\n",
    "\n",
    "        # --- Convert breakpoints (indices) -> depths (km)\n",
    "        bkps_depth = [z[min(end - 1, len(z) - 1)] for end in bkps if end <= len(z)]\n",
    "        bkps_all_km.append(bkps_depth)\n",
    "\n",
    "        # --- Build layers ---\n",
    "        vs_layers = []\n",
    "        h_layers = []\n",
    "        start = 0\n",
    "\n",
    "        for end in bkps:\n",
    "            segment_vs = vs[start:end]\n",
    "            segment_z = z[start:end]\n",
    "            mean_vs = np.mean(segment_vs)\n",
    "            vs_layers.append(mean_vs)\n",
    "            if len(segment_z) > 0:\n",
    "                h_layers.append(segment_z[-1] - segment_z[0])\n",
    "            else:\n",
    "                h_layers.append(0.0)\n",
    "            # end if\n",
    "            start = end\n",
    "        # end for\n",
    "\n",
    "        # Half-space (last layer)\n",
    "        if h_layers:\n",
    "            h_layers[-1] = 0.0\n",
    "        # end if\n",
    "\n",
    "        # Padding\n",
    "        n_layers = len(vs_layers)\n",
    "        h_padded = np.zeros(max_layers)\n",
    "        vs_padded = np.zeros(max_layers)\n",
    "        h_padded[:n_layers] = h_layers[:max_layers]\n",
    "        vs_padded[:n_layers] = vs_layers[:max_layers]\n",
    "\n",
    "        # Assemble θ vector\n",
    "        theta = [n_layers, vpvs] + h_padded.tolist() + vs_padded.tolist()\n",
    "        theta_all.append(theta)\n",
    "    # end for\n",
    "\n",
    "    theta_all = torch.tensor(theta_all, dtype=torch.float32)\n",
    "    penalties = np.array(penalties)\n",
    "\n",
    "    return theta_all, bkps_all_km, penalties\n",
    "# end def posterior_to_theta"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aaa71dfa",
   "metadata": {},
   "source": [
    "samples = posterior.sample((2000,), x=x_obs)\n",
    "print(f\"Sample shape: {samples.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5c97c598",
   "metadata": {},
   "source": [
    "theta_models, bkps, _ = posterior_to_theta(\n",
    "    z=z,\n",
    "    vs_batch=samples,\n",
    "    vpvs=1.75,\n",
    "    penalty_min=0.0,\n",
    "    penalty_max=15.0,\n",
    "    max_layers=layer_max\n",
    ")\n",
    "\n",
    "print(theta_models.shape)  # (10, 2 + 2*15)\n",
    "print(bkps[1])  # breakpoints du premier échantillon"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8e6b41f3",
   "metadata": {},
   "source": [
    "theta_models[1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aa852429",
   "metadata": {},
   "source": [
    "bkps[1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e2e2e5b5",
   "metadata": {},
   "source": [
    "# Plot the model and the curve\n",
    "plot_velocity_and_dispersion(\n",
    "    theta=theta_models[1],\n",
    "    disp_curve=x_obs[0],\n",
    "    p_min=p_min,\n",
    "    p_max=p_max,\n",
    "    kmax=kmax,\n",
    "    dpi=300\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "66ae323f7e109eeb",
   "metadata": {},
   "source": [
    "# Run the simulator on the sampled models\n",
    "disp_curves = dispsurf2k25_simulator(\n",
    "    theta=theta_models,\n",
    "    p_min=p_min,\n",
    "    p_max=p_max,\n",
    "    kmax=kmax,\n",
    "    iflsph=iflsph,\n",
    "    iwave=iwave,\n",
    "    mode=mode,\n",
    "    igr=igr,\n",
    "    progress=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0b0de818",
   "metadata": {},
   "source": [
    "disp_curves.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1aaeb92f",
   "metadata": {},
   "source": [
    "results = []\n",
    "for i in range(disp_curves.shape[0]):\n",
    "    # === erreur RMS ===\n",
    "    rms = np.sqrt(torch.mean((disp_curves[i].cpu() - x_obs[0].cpu())**2))\n",
    "    results.append({\n",
    "        \"disp_curve\": disp_curves[i],\n",
    "        \"rms\": rms\n",
    "    })\n",
    "# end for\n",
    "\n",
    "# Sort my RMS\n",
    "results_sorted = sorted(results, key=lambda r: r[\"rms\"])\n",
    "best = results_sorted[:20]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ba889e10",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,5), dpi=300)\n",
    "\n",
    "# Plot best curves\n",
    "for r in best:\n",
    "    plt.plot(periods, r[\"disp_curve\"], alpha=0.3)\n",
    "# end for\n",
    "\n",
    "# Plot the true dispersion curve\n",
    "plt.plot(periods, x_obs[0].cpu().numpy(), color='red', linewidth=3, label=\"Observation (x_obs)\")\n",
    "\n",
    "# Average misfit of the reconstructed dispersion curves\n",
    "mean_rms = np.mean([r[\"rms\"] for r in best])\n",
    "\n",
    "plt.xlim(p_min, p_max)\n",
    "plt.xlabel(\"Period (s)\")\n",
    "plt.ylabel(\"Phase velocity (km/s)\")\n",
    "plt.title(f\"Posterior Predictive Check - Dispersion curves\\nMean misfit (RMS) = {mean_rms:.3f}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "be81878d52bb9b63",
   "metadata": {},
   "source": [
    "### Posterior Predictive Distribution"
   ]
  },
  {
   "cell_type": "code",
   "id": "5da467248498b424",
   "metadata": {},
   "source": [
    "# quantiles\n",
    "q05 = np.percentile(disp_curves, 5, axis=0)\n",
    "q50 = np.percentile(disp_curves, 50, axis=0)\n",
    "q95 = np.percentile(disp_curves, 95, axis=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d26df5d6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "plt.figure(figsize=(10,5), dpi=300)\n",
    "plt.fill_between(periods, q05, q95, color=\"lightblue\", alpha=0.5, label=\"90% credible interval\")\n",
    "plt.plot(periods, q50, color=\"blue\", linewidth=2, label=\"Median prediction\")\n",
    "plt.plot(periods, x_obs[0].cpu().numpy(), color=\"red\", linewidth=2, label=\"Observation\")\n",
    "plt.xlabel(\"Period (s)\")\n",
    "plt.ylabel(\"Phase velocity (km/s)\")\n",
    "plt.title(\"Posterior Predictive Distribution (PPD) of dispersion curves\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "295f216fdfcbd22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Expected Coverage Probability and Posterior Calibration\n",
    "\n",
    "This cell evaluates the **Expected Coverage Probability (ECP)** of the posterior distribution — a standard diagnostic for **posterior calibration** in simulation-based inference (SBI).\n",
    "\n",
    "\n",
    "#### Practical insights\n",
    "\n",
    "Expected coverage provides a **simple and interpretable** way to diagnose issues in the posterior.   Compared to other diagnostics such as **L-C2ST**, it requires relatively few additional simulations (~200) and does **not** rely on extra hyperparameters (as **TARP** does) or additional neural network training.\n",
    "\n",
    "It allows us to evaluate whether the posterior is, *on average across many prior-predictive observations*,  \n",
    "**over-confident** or **under-confident**.\n",
    "\n",
    "![Texte alternatif](images/sbc_rank_plot.png)\n",
    "\n",
    "The plot can interpreted as follows:\n",
    "\n",
    "* The blue line is below the diagonal => then the posterior is (on average) over-confident.\n",
    "* The line is above the gray region => the posterior is, on average, under-confident.\n",
    "* The line is within the gray region => we cannot reject the null hypothesis that the posterior is well-calibrated.\n",
    "\n",
    "#### Method\n",
    "\n",
    "For each test observation $x_i$ with its corresponding ground-truth parameter $\\theta_i^\\ast$,  \n",
    "we draw multiple posterior samples $\\{ \\theta_{i,j} \\}_{j=1}^{N} \\sim p_\\phi(\\theta|x_i)$.  \n",
    "For a given credibility level $\\alpha$ (e.g. 0.9), we compute the **central credible interval** that covers $\\alpha\\%$ of the posterior mass:\n",
    "\n",
    "$$\n",
    "\\mathcal{I}_{\\alpha}(x_i) = \\Big[ q_{(1-\\alpha)/2}, \\; q_{1 - (1-\\alpha)/2} \\Big]\n",
    "$$\n",
    "\n",
    "Then we check whether the true parameter lies inside this interval:\n",
    "\n",
    "$$\n",
    "\\mathbf{1}\\big( \\theta_i^\\ast \\in \\mathcal{I}_{\\alpha}(x_i) \\big)\n",
    "$$\n",
    "\n",
    "Repeating this for all test observations gives the **empirical coverage**:\n",
    "\n",
    "$$\n",
    "\\hat{C}(\\alpha) = \\frac{1}{N_{\\text{obs}}} \\sum_i \\mathbf{1}\\big( \\theta_i^\\ast \\in \\mathcal{I}_{\\alpha}(x_i) \\big)\n",
    "$$\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- If the posterior is **well calibrated**, the observed coverage matches the expected credibility level:\n",
    "  $$\n",
    "  \\hat{C}(\\alpha) \\approx \\alpha\n",
    "  $$\n",
    "- If $\\hat{C}(\\alpha) < \\alpha$ → **overconfident posterior** (too narrow intervals).  \n",
    "- If $\\hat{C}(\\alpha) > \\alpha$ → **underconfident posterior** (too wide intervals).\n",
    "\n",
    "Plotting the **observed coverage** (y-axis) versus the **expected credibility levels** (x-axis) yields the *calibration curve*.  \n",
    "A perfectly calibrated model lies on the diagonal \\( y = x \\).\n",
    "\n",
    "#### Notes\n",
    "\n",
    "- The code computes this for several credibility levels (e.g. `[0.1, 0.3, 0.5, 0.7, 0.9]`).\n",
    "- For each level, it checks if the true parameter is entirely within the central credible interval (using `torch.quantile`).\n",
    "\n",
    "#### Reference\n",
    "\n",
    "Hermans, J., Delaunoy, A., Rozet, F., Wehenkel, A., & Louppe, G. (2022). [A trust crisis in simulation-based inference? Your posterior approximations can be unfaithful](https://arxiv.org/pdf/2110.06581). Machine Learning Research."
   ]
  },
  {
   "cell_type": "code",
   "id": "de883aa03188ce5a",
   "metadata": {},
   "source": [
    "levels = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "n_obs = min(100, x_val.shape[0])\n",
    "observed_counts = torch.zeros(len(levels), device=device)\n",
    "\n",
    "for i in range(n_obs):\n",
    "    # Get the obervation and the theta\n",
    "    x = x_val[i].unsqueeze(0).to(device)\n",
    "    theta_true = theta_val[i].to(device)\n",
    "    \n",
    "    # Draw 1000 samples from the posterior\n",
    "    samples = posterior.sample((1000,), x=x, show_progress_bars=False).to(device)\n",
    "    \n",
    "    # For each level\n",
    "    for k, alpha in enumerate(levels):\n",
    "        low = torch.quantile(samples, (1 - alpha) / 2, dim=0)\n",
    "        high = torch.quantile(samples, 1 - (1 - alpha) / 2, dim=0)\n",
    "        in_interval = ((theta_true >= low) & (theta_true <= high)).all().float()\n",
    "        observed_counts[k] += in_interval\n",
    "    # end for\n",
    "# end for\n",
    "\n",
    "# Compute ratio\n",
    "observed = observed_counts / n_obs\n",
    "expected = torch.tensor(levels, device=device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "81eb86cd",
   "metadata": {},
   "source": [
    "# --- PLOT ---\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.plot(expected.cpu(), observed.cpu(), \"o-\", label=\"Model\", linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect calibration\")\n",
    "plt.xlabel(\"Expected coverage\")\n",
    "plt.ylabel(\"Observed coverage\")\n",
    "plt.title(\"Posterior Calibration Curve (Expected vs Observed)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "45a1808f3d9730b2",
   "metadata": {},
   "source": [
    "### Visual summary"
   ]
  },
  {
   "cell_type": "code",
   "id": "6c5982aaf4c0b19d",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "12c48c6af04b225d",
   "metadata": {},
   "source": [
    "## Advanced Discussion — Sensitivity and Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc2173d31f4fef6",
   "metadata": {},
   "source": [
    "### Effect of the prior"
   ]
  },
  {
   "cell_type": "code",
   "id": "de4e648f72a5176e",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fb64bbd7ef2881d9",
   "metadata": {},
   "source": [
    "### Sequential methods"
   ]
  },
  {
   "cell_type": "code",
   "id": "c54e967984578a0f",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bcf07e826a953fef",
   "metadata": {},
   "source": [
    "### Out-of-distribution and generalization"
   ]
  },
  {
   "cell_type": "code",
   "id": "eb6d632a673f6b24",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
